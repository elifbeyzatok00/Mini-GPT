{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mini GPT Project"
      ],
      "metadata": {
        "id": "03QAZ99TGWQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gerekli paketleri yükle"
      ],
      "metadata": {
        "id": "SfrIdLalvMQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "786vKGbavNYv",
        "outputId": "d5711e6c-b556-491a-fa0a-eb71914981a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7FIuwl_jwkkF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU ayarı"
      ],
      "metadata": {
        "id": "1bCC4KlowsIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Kullanılan cihaz:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq46rctCwk7_",
        "outputId": "49be7976-d1e6-4134-8078-ea1279c0ed32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullanılan cihaz: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYPERPARAMETERS"
      ],
      "metadata": {
        "id": "6IvZZJIJw2uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 5000\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 100\n",
        "n_embed = 256\n",
        "n_heads = 8\n",
        "n_layers = 8\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "xavF8oCww4XC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Veri (input.txt)"
      ],
      "metadata": {
        "id": "43BZ3j86w-EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"input.txt\"\n",
        "if not os.path.exists(data_path):\n",
        "    # Küçük bir Shakespeare corpus indir\n",
        "    !wget -O input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    return \"\".join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data_source) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_source[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_source[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "0NCsD4yCxAUO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Blocks"
      ],
      "metadata": {
        "id": "kNYAAllOx0cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        # Mask is now in MultiHeadAttention, removed from here\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x is expected to be (B, T, head_size) from MultiHeadAttention after splitting heads\n",
        "        B,T,C = x.size()\n",
        "        k = self.key(x) # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # (B, T, T)\n",
        "\n",
        "        # Masking is now done in MultiHeadAttention\n",
        "\n",
        "        att = F.softmax(att, dim=-1) # (B, T, T)\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        out = att @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # Define and register the mask here\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size*num_heads, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.head_size = head_size # Store head_size\n",
        "        self.num_heads = num_heads # Store num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x is expected to be (B, T, n_embed)\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Apply Q, K, V linear layers and then split heads\n",
        "        qkv = nn.Linear(n_embed, 3 * n_embed, bias=False).to(x.device)(x) # (B, T, 3 * n_embed)\n",
        "        q, k, v = qkv.chunk(3, dim=-1) # Each (B, T, n_embed)\n",
        "\n",
        "        # Split heads\n",
        "        k = k.view(B, T, self.num_heads, self.head_size).transpose(1, 2) # (B, num_heads, T, head_size)\n",
        "        q = q.view(B, T, self.num_heads, self.head_size).transpose(1, 2) # (B, num_heads, T, head_size)\n",
        "        v = v.view(B, T, self.num_heads, self.head_size).transpose(1, 2) # (B, num_heads, T, head_size)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        # (q @ k.transpose(-2, -1)) * (1.0 / sqrt(head_size))\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_size)) # (B, num_heads, T, T)\n",
        "\n",
        "        # Apply mask\n",
        "        mask_sliced = self.mask[:, :, :T, :T]\n",
        "        att = att.masked_fill(mask_sliced == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1) # (B, num_heads, T, T)\n",
        "        att = self.dropout(att)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = att @ v # (B, num_heads, T, head_size)\n",
        "\n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, self.num_heads * self.head_size) # (B, T, n_embed)\n",
        "\n",
        "        # Linear projection\n",
        "        out = self.dropout(self.proj(out)) # (B, T, n_embed)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input and output shape should be (B, T, n_embed)\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_heads\n",
        "        self.sa = MultiHeadAttention(n_heads, head_size)\n",
        "        self.ff = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input and output shape should be (B, T, n_embed)\n",
        "        # Residual connection 1\n",
        "        x = x + self.sa(self.ln1(x)) # (B, T, n_embed) + (B, T, n_embed) -> (B, T, n_embed)\n",
        "\n",
        "        # Residual connection 2\n",
        "        x = x + self.ff(self.ln2(x)) # (B, T, n_embed) + (B, T, n_embed) -> (B, T, n_embed)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PczFJH7ex1_Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "HPYd208nx5Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.size()\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C = logits.size()\n",
        "            loss = F.cross_entropy(logits.view(B*T,C), targets.view(B*T))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                minv = v[:, -1].unsqueeze(1)\n",
        "                logits = torch.where(logits < minv, torch.full_like(logits, -1e10), logits)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_id), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = MiniGPT().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "G0pIp4otx6vG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss tahmini"
      ],
      "metadata": {
        "id": "SVvzUkB3x9Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train','val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            xb,yb = get_batch(split)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "y6BY9H8_x-L5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "TT0RPYRtyAJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "best_val = 1e9\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter} | train loss {losses['train']:.4f} | val loss {losses['val']:.4f} | time {(time.time()-start_time):.1f}s\")\n",
        "        context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "        sample = model.generate(context, max_new_tokens=200, temperature=1.0, top_k=10)[0].tolist()\n",
        "        print(\"---- sample ----\")\n",
        "        print(decode(sample))\n",
        "        print(\"----------------\")\n",
        "    if losses['val'] < best_val:\n",
        "        best_val = losses['val']\n",
        "        torch.save(model.state_dict(), \"mini_gpt_checkpoint.pt\")\n",
        "    xb,yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training finished.\")\n",
        "torch.save(model.state_dict(), \"mini_gpt_final.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuEFboVkyBO3",
        "outputId": "339559b8-f6c6-4d7c-bb35-8f313d49336e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | train loss 4.3726 | val loss 4.3639 | time 6.1s\n",
            "---- sample ----\n",
            "\n",
            "eCeNW:KlSQbRuSg$$TzZDSRfzvw;WGoNOLGoXSLSszIvXYPQCKduvHO&WxrmZ3YzDIFJpbNwnA!FWBnAMPtghbgivZqwpFEHSWTkgegiyiV?ivZFRu-,qoIDcMa,ixcf3\n",
            "G;W,L,Ee!KPItm3e:ILdaIEL!m3Gw;Im?bGF,Lm!mEP?PI?!;,PGGGdG$eJEKWGF?hmKWG\n",
            "----------------\n",
            "step 200 | train loss 2.4983 | val loss 2.5149 | time 31.4s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "\n",
            "Wat l be s thourou thealathire thanof berd sthinoupomesth therthanghit mers me th tare nt anofofo s mend,\n",
            "AMINCEETor d.\n",
            "INGon wh thind hatth o ded t weathat, she tins mousoureres asitheng, owere of \n",
            "----------------\n",
            "step 400 | train loss 2.4795 | val loss 2.4985 | time 57.0s\n",
            "---- sample ----\n",
            "\n",
            "Th ayouthy h athon, we core he t thowithe I fowadiss se an breren he the, hiroun, ot wa hin tomande wis ime ped tayo cele he hato th, hes se whisthees, morte son wh tinter he s co ssthat o ise ht te m\n",
            "----------------\n",
            "step 600 | train loss 2.4711 | val loss 2.4950 | time 82.1s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "INI imeave t fomothusathes thishas hithas thentet, tits ot ay hel t o fantr, tim benghim ased t id s the,\n",
            "\n",
            "Sed, buste al, othier m it incore ang thildor, t bleath hatofat spe tarenthoumeerin st s dou\n",
            "----------------\n",
            "step 800 | train loss 2.4707 | val loss 2.4925 | time 107.1s\n",
            "---- sample ----\n",
            "\n",
            "SThoo mpr hilala at.\n",
            "IUETe atr s inde boweres we br, t horous s,\n",
            "HIOR:\n",
            "Tor h te, wor al, wancu aner fe withour a ais,\n",
            "WAnghe womus thtond bount te, homaionde thand an way thonen se it, wothe,\n",
            "I heatou\n",
            "----------------\n",
            "step 1000 | train loss 2.4714 | val loss 2.4990 | time 132.6s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "\n",
            "Thanore ce d sese tom thy alind heses, d car, tathithe mers o ffong more,\n",
            "Be and ban hy hand wher t imere t, wo ssior w thy; hatom be strto s ongurd s te s, hora hthoow har tisure fouro he t de, tht\n",
            "----------------\n",
            "step 1200 | train loss 2.4704 | val loss 2.4909 | time 158.4s\n",
            "---- sample ----\n",
            "\n",
            "COUKIns thed muthel livind antaved, anoull alil, w htoronchour, hor divicoull an wiloucrd blaryouringine hinist\n",
            "A he s thachourtistom, istowhanoures he,\n",
            "\n",
            "Thofo th sise dimpre ad thine wssorais war, be\n",
            "----------------\n",
            "step 1400 | train loss 2.4667 | val loss 2.4873 | time 183.9s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "I s in fthers the is te mally theans wasththeshiste o bouth w the, mend antor h werdisth hul atidicharsos,\n",
            "Ser hadat tir housta tineathes out s mes buss want w he m the me ies theran be sh friman, is\n",
            "----------------\n",
            "step 1600 | train loss 2.4645 | val loss 2.4901 | time 209.3s\n",
            "---- sample ----\n",
            "\n",
            "Fowil muponthayer frien setan f al tre isuru on t sedenou t an the, so t m s ist art s histilline be f mborsatrimy\n",
            "He leandorow arend heat hes ader wies t my berer s,\n",
            "Whthedouserigeto than houlfererer\n",
            "----------------\n",
            "step 1800 | train loss 2.4642 | val loss 2.4865 | time 234.4s\n",
            "---- sample ----\n",
            "\n",
            "THat, wiesuld hesates tomun t shy malowor,\n",
            "\n",
            "\n",
            "\n",
            "And mer t here here cily harsearielilllfasered,\n",
            "I o t orshe sthurofonghe hr atomad are whousooureano iothy bl wathitothangre t tou ords ound m bit, teshin\n",
            "----------------\n",
            "step 2000 | train loss 2.4616 | val loss 2.4903 | time 259.6s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "IO bly, that hour omeando myootonetof ins at half the hathe m mpind bes t willle teld.\n",
            "S: t thende f the he l d, me foso ala mee tre mue mbeamenootad t finghe, t al t hest se t hof tr,\n",
            "Hedises\n",
            "BENCED\n",
            "----------------\n",
            "step 2200 | train loss 2.4629 | val loss 2.4909 | time 284.7s\n",
            "---- sample ----\n",
            "\n",
            "ANUSetrd imanilen fr tat wod, medichin s he and wo blfughee ooreee alllitayetan ote he hin tound su t thalareeraritr t, st he be t te tes otrotor, se isis tound an t withangathadayowilend\n",
            "\n",
            "Fo trenen, \n",
            "----------------\n",
            "step 2400 | train loss 2.4633 | val loss 2.4909 | time 310.3s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "Busimbul my my s he a harof athat m,\n",
            "Ang sily, w ith tthorat fe am ouclis medou s s sheriloner har,\n",
            "STh towed tharay\n",
            "He t,\n",
            "\n",
            "Anthiman, thantonthrowathen anis hy su hur this fie imy be wathe wod wha my\n",
            "----------------\n",
            "step 2600 | train loss 2.4623 | val loss 2.4831 | time 336.2s\n",
            "---- sample ----\n",
            "\n",
            "N t s ot, therootitlowr tindor bl sethyo he, m fe war wollede anorethe he, ine he mor s hand malal otr s the han buserear ithe, th,\n",
            "\n",
            "\n",
            "\n",
            "WAsice isenon trs aldrand d m s athasend mbl as th feerous fus, i\n",
            "----------------\n",
            "step 2800 | train loss 2.4638 | val loss 2.4831 | time 361.6s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "A t ire my or inghe the,\n",
            "HE:\n",
            "\n",
            "MERINIClllilde hicthir wat y hit sent, whe sent arayererinome birseancloticllllin the t sisour tho lad ast mave thad, tl brises waillisioom t\n",
            "I the fare, ave bl thangore\n",
            "----------------\n",
            "step 3000 | train loss 2.4607 | val loss 2.4882 | time 386.7s\n",
            "---- sample ----\n",
            "\n",
            "HOMI wieave the,\n",
            "\n",
            "Honancough bt be althe theesarinomurer thalds a tino warea oue avithas bisoring or willl than heat me, m my thed thait honouncureang melor may t t s m theng ter so mes har s wirtoun \n",
            "----------------\n",
            "step 3200 | train loss 2.4589 | val loss 2.4848 | time 411.8s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "MENTIO:\n",
            "\n",
            "Whed thoury, mier ot, as,\n",
            "S:\n",
            "Medithe ionderthilind wsounonou f f tor te bllilenes thth oust,\n",
            "\n",
            "ABus be s icee of averartheanser bee aliomas, wen f fo thay adinill t thellir ise whitor tho s w\n",
            "----------------\n",
            "step 3400 | train loss 2.4580 | val loss 2.4904 | time 436.8s\n",
            "---- sample ----\n",
            "\n",
            "Thouled hongo my t othis thttth beneroul hen se the,\n",
            "\n",
            "Thelotesithim s hard fomelimat w astin,\n",
            "T: meshe se whas whit t s hesered methes, whe mes so s and wn ar mpr, orst w omor ther,\n",
            "\n",
            "Tosse heaindie he\n",
            "----------------\n",
            "step 3600 | train loss 2.4605 | val loss 2.4879 | time 461.9s\n",
            "---- sample ----\n",
            "\n",
            "Asie st ayomy youses s t odoun ware ar withe y itothed thie thil thethethid tand be in thino isppe thistarour t, thearisone s s t merourthirthe we sithathad ath, owitest war a he s orom ilethatheand w\n",
            "----------------\n",
            "step 3800 | train loss 2.4629 | val loss 2.4870 | time 487.3s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "Whay asas in mbr thim teres thed andied thas merthar hicurese howitathe mp manthind\n",
            "T:\n",
            "ANCones, be cond here ad hothor s ainou bouthainel he,\n",
            "Whirderous wavero owo s toncl fende t frou t cree, t tave\n",
            "----------------\n",
            "step 4000 | train loss 2.4592 | val loss 2.4896 | time 513.1s\n",
            "---- sample ----\n",
            "\n",
            "\n",
            "An isonst mend athe waretelongisule whe thean hanous ss blts ais frt mangild m t brethou and iroravishan, thisel s bldaspors tomen the sheste ilare thime ofoucasave me t tha the, we or in th is bo or\n",
            "----------------\n",
            "step 4200 | train loss 2.4570 | val loss 2.4883 | time 538.7s\n",
            "---- sample ----\n",
            "\n",
            "Bee an foutoue wenge hingh wioou o the owireanoure tonorse toucord as shouchyomiesery f s mbsthararen,\n",
            "Beashald, ithilimyowe anthouenoung,\n",
            "\n",
            "BEENTingethish the whe inil buguee sisso hourarinds tayothe \n",
            "----------------\n",
            "step 4400 | train loss 2.4598 | val loss 2.4860 | time 563.8s\n",
            "---- sample ----\n",
            "\n",
            "S:\n",
            "I sherire there, sould,\n",
            "Tiss,\n",
            "I'der mee t or, omust s assha ou mith w, hyonsierisome omy im sth to isour,\n",
            "Se he ilese witst thave th al at im ses fonear soro wend hee a fil withit bllt t thald ofrs\n",
            "----------------\n",
            "step 4600 | train loss 2.4595 | val loss 2.4851 | time 588.9s\n",
            "---- sample ----\n",
            "\n",
            "T:\n",
            "ANGot monour s t heeres bean wand mas thousthten s s m,\n",
            "THAn, ole ingould d t t hee t w s titrt thyomy anged ithay d d ono shatorilf s thalsuthatoontr heet he h t touren to icuche al he oofrthe wis\n",
            "----------------\n",
            "step 4800 | train loss 2.4583 | val loss 2.4857 | time 614.0s\n",
            "---- sample ----\n",
            "\n",
            "Fand w ithththera andiend he soficonchonousildrand shimot oursther,\n",
            "AMy.\n",
            "\n",
            "ONTHere t mbeler ar tounde aneerelle he,\n",
            "Whe harerotherond te,\n",
            "Thitharts f thies or, w dint owisust thalad heet blful bureesse\n",
            "----------------\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text üretme fonksiyonu"
      ],
      "metadata": {
        "id": "2nzms6ikyDrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_from_file(prefix, length=400, temp=1.0, top_k=20):\n",
        "  model.eval()\n",
        "  context = torch.tensor(encode(prefix), device=device, dtype=torch.long).unsqueeze(0)\n",
        "  out_ids = model.generate(context, max_new_tokens=length, temperature=temp, top_k=top_k)[0].tolist()\n",
        "  return decode(out_ids)\n",
        "\n",
        "print(\"\\n--- Final sample generation ---\")\n",
        "sample_text = generate_from_file(\"The\", length=300, temp=0.8, top_k=10)\n",
        "print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvj1-hQMyEvk",
        "outputId": "1b8b6e2e-bdd9-4808-f971-1f1070fdc617"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final sample generation ---\n",
            "The has thens,\n",
            "HAnd, woote all sh alothe heng h sensered at bet mitheree s, for f se is, thary s thoureenowangusen, thathis the t waras that totore taith artowond tour, aveane tr boreld fo fris, t the ts ang manoms s me owir me tind stofte hellathadis te thend, o myof hieno he allisthowin asis byor s t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Çıktıyı İyileştirmek İçin Yapılabilecekler\n",
        "\n",
        "\n",
        "1. **Model Boyutunu Artırma**\n",
        "\n",
        "   * `n_embed`: **128 → 256 veya 384**\n",
        "   * `n_heads`: **4 → 6 veya 8**\n",
        "   * `n_layers`: **4 → 6 veya 8**\n",
        "\n",
        "2. **Daha Fazla Eğitim**\n",
        "\n",
        "   * `max_iters`: **2000 → 5000–10000**\n",
        "\n",
        "3. **Öğrenme Oranını Ayarlama**\n",
        "\n",
        "   * `learning_rate`: **3e-4 (mevcut)**\n",
        "\n",
        "     * Alternatif denemeler: **1e-4, 2e-4, 5e-4**\n",
        "\n",
        "4. **Daha Büyük Veri Kümesi Kullanma**\n",
        "\n",
        "   * Tiny Shakespeare (~1MB) yerine:\n",
        "\n",
        "     * **Wikipedia makaleleri (10–100MB)**\n",
        "     * **Kitaplar (Project Gutenberg, birkaç MB–GB)**\n",
        "     * **Kendi notlarınız / domain spesifik veriler**\n",
        "\n",
        "5. **Gelişmiş Eğitim Teknikleri**\n",
        "\n",
        "   * **LR Scheduler**: `torch.optim.lr_scheduler.CosineAnnealingLR`\n",
        "\n",
        "     * Örn: başlangıç **3e-4**, minimum **1e-5**\n",
        "   * **Weight Decay (düzenlileştirme)**:\n",
        "\n",
        "     * `optimizer = AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)`\n"
      ],
      "metadata": {
        "id": "BR9Nad4HGuxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaynaklar\n",
        "\n",
        "https://github.com/karpathy/nanoGPT\n",
        "\n",
        "https://huggingface.co/docs/transformers/en/index\n",
        "\n",
        "https://chatgpt.com/"
      ],
      "metadata": {
        "id": "kbYA1TcJGDLQ"
      }
    }
  ]
}